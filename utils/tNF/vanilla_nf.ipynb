{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "083172a8",
   "metadata": {},
   "source": [
    "# Vanilla NF Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d48b57",
   "metadata": {},
   "source": [
    "02/28/2023. We replicate the implementations of RealNVP and ResNet and perform density estimation tasks. In this notebook, no time dependence is assumed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8183f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.nn.init as init\n",
    "import torch.distributions.transforms as transform\n",
    "import torch.nn.functional as functional\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_default_dtype(torch.float64)\n",
    "# set random seed\n",
    "SEED_ = 10\n",
    "np.random.seed(SEED_)\n",
    "torch.manual_seed(SEED_)\n",
    "\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine_Coupling(nn.Module):\n",
    "    def __init__(self, mask, hidden_dim):\n",
    "        super(Affine_Coupling, self).__init__()\n",
    "        self.input_dim = len(mask)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        ## mask to seperate positions that do not change and positions that change.\n",
    "        ## mask[i] = 1 means the ith position does not change.\n",
    "        self.mask = nn.Parameter(mask, requires_grad = False)\n",
    "\n",
    "        ## layers used to compute scale in affine transformation\n",
    "        self.scale_fc1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.scale_fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.scale_fc3 = nn.Linear(self.hidden_dim, self.input_dim)\n",
    "        self.scale = nn.Parameter(torch.Tensor(self.input_dim))\n",
    "        init.normal_(self.scale)\n",
    "\n",
    "        ## layers used to compute translation in affine transformation \n",
    "        self.translation_fc1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.translation_fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.translation_fc3 = nn.Linear(self.hidden_dim, self.input_dim)\n",
    "\n",
    "    def _compute_scale(self, x):\n",
    "        ## compute scaling factor using unchanged part of x with a neural network\n",
    "        s = torch.relu(self.scale_fc1(x*self.mask))\n",
    "        s = torch.relu(self.scale_fc2(s))\n",
    "        s = torch.relu(self.scale_fc3(s)) * self.scale        \n",
    "        return s\n",
    "\n",
    "    def _compute_translation(self, x):\n",
    "        ## compute translation using unchanged part of x with a neural network        \n",
    "        t = torch.relu(self.translation_fc1(x*self.mask))\n",
    "        t = torch.relu(self.translation_fc2(t))\n",
    "        t = self.translation_fc3(t)        \n",
    "        return t\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## convert latent space variable to observed variable\n",
    "        s = self._compute_scale(x)\n",
    "        t = self._compute_translation(x)\n",
    "        \n",
    "        y = self.mask*x + (1-self.mask)*(x*torch.exp(s) + t)        \n",
    "        logdet = torch.sum((1 - self.mask)*s, -1)\n",
    "        \n",
    "        return y, logdet\n",
    "\n",
    "    def inverse(self, y):\n",
    "        ## convert observed varible to latent space variable\n",
    "        s = self._compute_scale(y)\n",
    "        t = self._compute_translation(y)\n",
    "                \n",
    "        x = self.mask*y + (1-self.mask)*((y - t)*torch.exp(-s))\n",
    "        logdet = torch.sum((1 - self.mask)*(-s), -1)\n",
    "        \n",
    "        return x, logdet\n",
    "\n",
    "    \n",
    "class RealNVP_2D(nn.Module):\n",
    "    '''\n",
    "    A vanilla RealNVP class for modeling 2 dimensional distributions\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, masks, hidden_dim):\n",
    "        '''\n",
    "        initialized with a list of masks. each mask define an affine coupling layer\n",
    "        '''\n",
    "        super(RealNVP_2D, self).__init__()        \n",
    "        self.hidden_dim = hidden_dim        \n",
    "        self.masks = nn.ParameterList(\n",
    "            [nn.Parameter(torch.Tensor(m),requires_grad = False)\n",
    "             for m in masks])\n",
    "\n",
    "        self.affine_couplings = nn.ModuleList(\n",
    "            [Affine_Coupling(self.masks[i], self.hidden_dim)\n",
    "             for i in range(len(self.masks))])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## convert latent space variables into observed variables\n",
    "        y = x\n",
    "        logdet_tot = 0\n",
    "        for i in range(len(self.affine_couplings)):\n",
    "            y, logdet = self.affine_couplings[i](y)\n",
    "            logdet_tot = logdet_tot + logdet\n",
    "\n",
    "        ## a normalization layer is added such that the observed variables is within\n",
    "        ## the range of [-4, 4].\n",
    "        logdet = torch.sum(torch.log(torch.abs(4*(1-(torch.tanh(y))**2))), -1)        \n",
    "        y = 4*torch.tanh(y)\n",
    "        logdet_tot = logdet_tot + logdet\n",
    "        \n",
    "        return y, logdet_tot\n",
    "\n",
    "    def inverse(self, y):\n",
    "        ## convert observed variables into latent space variables        \n",
    "        x = y        \n",
    "        logdet_tot = 0\n",
    "\n",
    "        # inverse the normalization layer\n",
    "        logdet = torch.sum(torch.log(torch.abs(1.0/4.0* 1/(1-(x/4)**2))), -1)\n",
    "        x  = 0.5*torch.log((1+x/4)/(1-x/4))\n",
    "        logdet_tot = logdet_tot + logdet\n",
    "\n",
    "        ## inverse affine coupling layers\n",
    "        for i in range(len(self.affine_couplings)-1, -1, -1):\n",
    "            x, logdet = self.affine_couplings[i].inverse(x)\n",
    "            logdet_tot = logdet_tot + logdet\n",
    "            \n",
    "        return x, logdet_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead42059",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Masks used to define the number and the type of affine coupling layers\n",
    "## In each mask, 1 means that the variable at the correspoding position is\n",
    "## kept fixed in the affine couling layer\n",
    "masks = [[1.0, 0.0],\n",
    "         [0.0, 1.0],\n",
    "         [1.0, 0.0],         \n",
    "         [0.0, 1.0],\n",
    "         [1.0, 0.0],         \n",
    "         [0.0, 1.0],\n",
    "         [1.0, 0.0],\n",
    "         [0.0, 1.0]]\n",
    "\n",
    "## dimenstion of hidden units used in scale and translation transformation\n",
    "hidden_dim = 128\n",
    "\n",
    "## construct the RealNVP_2D object\n",
    "realNVP = RealNVP_2D(masks, hidden_dim)\n",
    "if torch.cuda.device_count():\n",
    "    realNVP = realNVP.cuda()\n",
    "device = next(realNVP.parameters()).device\n",
    "\n",
    "optimizer = torch.optim.Adam(realNVP.parameters(), lr = 0.0001)\n",
    "num_steps = 10000\n",
    "\n",
    "## the following loop learns the RealNVP_2D model by data\n",
    "## in each loop, data is dynamically sampled from the scipy moon dataset\n",
    "for idx_step in range(num_steps):\n",
    "    ## sample data from the scipy moon dataset\n",
    "    X, label = datasets.make_moons(n_samples = 512, noise = 0.05)\n",
    "    X = torch.Tensor(X).to(device = device)\n",
    "\n",
    "    ## transform data X to latent space Z\n",
    "    z, logdet = realNVP.inverse(X)\n",
    "\n",
    "    ## calculate the negative loglikelihood of X\n",
    "    loss = torch.log(z.new_tensor([2*np.pi])) + torch.mean(torch.sum(0.5*z**2, -1) - logdet)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    if (idx_step + 1) % 100 == 0:\n",
    "        print(f\"idx_steps: {idx_step:}, loss: {loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0205f03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## after learning, we can test if the model can transform\n",
    "## the moon data distribution into the normal distribution\n",
    "X, label = datasets.make_moons(n_samples = 1000, noise = 0.05)\n",
    "X = torch.Tensor(X).to(device = device)\n",
    "z, logdet_jacobian = realNVP.inverse(X)\n",
    "z = z.cpu().detach().numpy()\n",
    "\n",
    "X = X.cpu().detach().numpy()\n",
    "fig = plt.figure(2, figsize = (12.8, 4.8))\n",
    "fig.clf()\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(X[label==0,0], X[label==0,1], \".\")\n",
    "plt.plot(X[label==1,0], X[label==1,1], \".\")\n",
    "plt.title(\"X sampled from Moon dataset\")\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(z[label==0,0], z[label==0,1], \".\")\n",
    "plt.plot(z[label==1,0], z[label==1,1], \".\")\n",
    "plt.title(\"Z transformed from X\")\n",
    "plt.xlabel(r\"$z_1$\")\n",
    "plt.ylabel(r\"$z_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e671804",
   "metadata": {},
   "outputs": [],
   "source": [
    "## after learning, we can also test if the model can transform\n",
    "## the normal distribution into the moon data distribution \n",
    "z = torch.normal(0, 1, size = (1000, 2)).to(device = device)\n",
    "X, _ = realNVP(z)\n",
    "X = X.cpu().detach().numpy()\n",
    "z = z.cpu().detach().numpy()\n",
    "\n",
    "fig = plt.figure(2, figsize = (12.8, 4.8))\n",
    "fig.clf()\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(z[:,0], z[:,1], \".\")\n",
    "plt.title(\"Z sampled from normal distribution\")\n",
    "plt.xlabel(r\"$z_1$\")\n",
    "plt.ylabel(r\"$z_2$\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(X[:,0], X[:,1], \".\")\n",
    "plt.title(\"X transformed from Z\")\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54fc888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample grid and compute density estimates\n",
    "\n",
    "# latent grid\n",
    "x1_min, x1_max = -3.0, 3.0\n",
    "x2_min, x2_max = -3.0, 3.0\n",
    "N = 200\n",
    "x1_grid = np.linspace(x1_min, x1_max, N)\n",
    "x2_grid = np.linspace(x2_min, x2_max, N)\n",
    "# meshgrid\n",
    "x1_mesh, x2_mesh = np.meshgrid(x1_grid, x2_grid)\n",
    "# get list of coordinates\n",
    "x_data = np.concatenate((x1_mesh.ravel().reshape(-1,1), x2_mesh.ravel().reshape(-1,1)), axis=1)\n",
    "x_data = torch.tensor(x_data)\n",
    "# compute density using normalizing flow\n",
    "z_data, jac = realNVP.inverse(x_data)\n",
    "# evaluate density\n",
    "p_x = torch.exp(-(torch.sum(0.5*z_data**2, -1) - jac + torch.log(2*torch.tensor(torch.pi)))).reshape(N, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed5c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_x = p_x.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf77a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(p_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea26bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
