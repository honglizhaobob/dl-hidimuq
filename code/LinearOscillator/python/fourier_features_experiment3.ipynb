{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36233a32",
   "metadata": {},
   "source": [
    "# Regression with Fourier Features - Part 3\n",
    "\n",
    "Apply the space-time Fourier embedded net on the original linear oscillator problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe062395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "from collections import OrderedDict\n",
    "\n",
    "# set random seeds\n",
    "np.random.seed(10)\n",
    "torch.manual_seed(10);\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "import numpy as np    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# import neural nets\n",
    "from PINN.utils.dnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92445366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1000) (1, 1000) (1, 1000)\n"
     ]
    }
   ],
   "source": [
    "# modify dataset\n",
    "full_data_path = \"../data/LinearOscillator/OU_Noise_energy.mat\"\n",
    "data = scipy.io.loadmat(full_data_path)\n",
    "# subsample factor\n",
    "space_factor = 5\n",
    "time_factor = 5\n",
    "new_pmc = (data[\"v_density\"].T)[0:-1:time_factor, 0:-1:space_factor]\n",
    "new_xgrid = data[\"xi\"][:, 0:-1:space_factor]\n",
    "new_tgrid = data[\"tspan\"][:, 0:-1:time_factor]\n",
    "# save new data\n",
    "new_data_path = \"../data/LinearOscillator/OU_Noise_energy_subsample{}.mat\".format(int(space_factor*time_factor))\n",
    "scipy.io.savemat(\n",
    "    new_data_path, {\"pmc\": new_pmc, \"xgrid\": new_xgrid, \"tgrid\": new_tgrid}\n",
    ")\n",
    "# test saved data\n",
    "new_data_path = \"../data/LinearOscillator/OU_Noise_energy_subsample25.mat\"\n",
    "data = scipy.io.loadmat(new_data_path)\n",
    "print(data[\"pmc\"].shape, data[\"xgrid\"].shape, data[\"tgrid\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "defffc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds\n",
    "np.random.seed(10)\n",
    "torch.manual_seed(10);\n",
    "\n",
    "data_path = \"../data/LinearOscillator/OU_Noise_energy_subsample25.mat\"\n",
    "data = scipy.io.loadmat(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bab08754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.contour.QuadContourSet at 0x7f9beea61580>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize data\n",
    "tmp = torch.clamp(torch.log(torch.tensor(data[\"pmc\"])), -30.0)\n",
    "plt.contourf(tmp.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7822fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cartesian_data(torch.tensor(new_tgrid[0].flatten()), torch.tensor(new_xgrid.flatten()))\n",
    "y = torch.tensor(new_pmc).T.flatten().reshape(-1, 1)\n",
    "# learn on log scale\n",
    "log_scale = True\n",
    "if log_scale:\n",
    "    y = torch.clamp(torch.log(y), -30.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b60f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "normalize = True\n",
    "if normalize:\n",
    "    X = X / X.mean(dim=0)\n",
    "    y = y / y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17a27423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training function\n",
    "def train(inputs, outputs, model, optim, scheduler, batch_size, epochs, shuffle=True):\n",
    "    X, y = inputs, outputs\n",
    "    nx = X.shape[0]\n",
    "    num_batches = int(nx/batch_size)\n",
    "    for i in range(epochs):\n",
    "        print(\"============================================================\\n\")\n",
    "        print(\"Epoch = {}\\n\".format(i+1));\n",
    "        print(\"============================================================\\n\")\n",
    "        model.train()\n",
    "        if shuffle:\n",
    "            tmp = np.random.permutation(nx)\n",
    "            X, y = X[tmp, :].data.clone(), y[tmp, :].data.clone()\n",
    "        for idx in range(num_batches):\n",
    "            if idx % 100 == 0:\n",
    "                print(\"| => | Batch {} |\\n\".format(idx+1))\n",
    "        # closure definition\n",
    "            def closure():\n",
    "                optim.zero_grad()\n",
    "                start_idx = idx*batch_size\n",
    "                end_idx = (idx+1)*batch_size\n",
    "                if idx + 1 == num_batches:\n",
    "                    # if last batch\n",
    "                    end_idx = -1\n",
    "                Xb, yb = X[start_idx:end_idx, :].data.clone(), y[start_idx:end_idx, :].data.clone()\n",
    "\n",
    "                # require gradients\n",
    "                Xb.requires_grad = True\n",
    "                # make a prediction on the batch\n",
    "                y_pred = model.forward(Xb)\n",
    "                # compute L^2 loss\n",
    "                loss = torch.mean((y_pred - yb)**2)\n",
    "                # backpropagate\n",
    "                loss.backward()\n",
    "                if idx % 100 == 0:\n",
    "                    print(\"==> Batch {} loss = {}\".format(idx, loss.item()))\n",
    "                return loss\n",
    "            optim.step(closure=closure)\n",
    "        if scheduler:\n",
    "            # step scheduler after epoch if there is one\n",
    "            scheduler.step()\n",
    "            print(\"---------- \\n\")\n",
    "            print(\"++ Learning rate reduced, now at = {}\".format(scheduler.get_last_lr()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dced14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "Epoch = 1\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 1.0210247608850942\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.01821023525133255\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.010383969605894215\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.0079992\n",
      "============================================================\n",
      "\n",
      "Epoch = 2\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.01039005934285005\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.009740629863792732\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.008910397369153916\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007998400079999999\n",
      "============================================================\n",
      "\n",
      "Epoch = 3\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0068156878835752156\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0073662494762841875\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.006027566427570755\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007997600239991999\n",
      "============================================================\n",
      "\n",
      "Epoch = 4\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.005655186046039411\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.005823714873065537\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.004595590885859549\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007996800479968\n",
      "============================================================\n",
      "\n",
      "Epoch = 5\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0043657538039474555\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.004612391819066551\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.00394239571582789\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007996000799920003\n",
      "============================================================\n",
      "\n",
      "Epoch = 6\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.003698696741127382\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.003940523714380209\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0042164335965105566\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007995201199840011\n",
      "============================================================\n",
      "\n",
      "Epoch = 7\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0037333927115774405\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0032737722347977037\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0035807772181959035\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007994401679720027\n",
      "============================================================\n",
      "\n",
      "Epoch = 8\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.003020480878168649\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.003279111468314321\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.003168693761973684\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007993602239552054\n",
      "============================================================\n",
      "\n",
      "Epoch = 9\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0029430779161797845\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0032146267610027946\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0027931165620423207\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007992802879328098\n",
      "============================================================\n",
      "\n",
      "Epoch = 10\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.002716273043766004\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0029001058221030116\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0027723586563205985\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007992003599040166\n",
      "============================================================\n",
      "\n",
      "Epoch = 11\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0032781270625783683\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0026654973532952136\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0035738742423741493\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007991204398680262\n",
      "============================================================\n",
      "\n",
      "Epoch = 12\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.00293322241528124\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.002531918492568316\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0027575411707443046\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007990405278240395\n",
      "============================================================\n",
      "\n",
      "Epoch = 13\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0022192913222119745\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.002946946878581025\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0024250874367979904\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007989606237712572\n",
      "============================================================\n",
      "\n",
      "Epoch = 14\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0025320172956199496\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0023329567546637583\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0021562877114914177\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.0079888072770888\n",
      "============================================================\n",
      "\n",
      "Epoch = 15\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0019281729460045873\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0022103354170110077\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0023879253027834432\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007988008396361091\n",
      "============================================================\n",
      "\n",
      "Epoch = 16\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.00226996324335862\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0021277171884372245\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.00198613857885332\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007987209595521455\n",
      "============================================================\n",
      "\n",
      "Epoch = 17\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0018780363627590604\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.00204621291810089\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0018497370528297143\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007986410874561903\n",
      "============================================================\n",
      "\n",
      "Epoch = 18\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0022950428030489487\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.00200411444077311\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0016804970621969427\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007985612233474448\n",
      "============================================================\n",
      "\n",
      "Epoch = 19\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.002064390873523606\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0017629185476148224\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0018415003665602785\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.0079848136722511\n",
      "============================================================\n",
      "\n",
      "Epoch = 20\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.001900851905550397\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0020215665479253425\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0020903955108472665\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007984015190883875\n",
      "============================================================\n",
      "\n",
      "Epoch = 21\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0021571765796892304\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.002064702655085548\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.00193584561803726\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007983216789364787\n",
      "============================================================\n",
      "\n",
      "Epoch = 22\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Batch 0 loss = 0.0018419767492365699\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0019240608372172187\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.001900433356417333\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.00798241846768585\n",
      "============================================================\n",
      "\n",
      "Epoch = 23\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0016893862215517927\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0016962376634897247\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.001965608417815464\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007981620225839082\n",
      "============================================================\n",
      "\n",
      "Epoch = 24\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0019526321856094602\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0018447177522086992\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0018892699017446829\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007980822063816497\n",
      "============================================================\n",
      "\n",
      "Epoch = 25\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.001809215681422067\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0018490551578076962\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.001613066421814126\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007980023981610115\n",
      "============================================================\n",
      "\n",
      "Epoch = 26\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0018354398096429453\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0017454496695237128\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0016491351020386665\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007979225979211954\n",
      "============================================================\n",
      "\n",
      "Epoch = 27\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0017530145262462929\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.001790446343540551\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0017097691308815741\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007978428056614032\n",
      "============================================================\n",
      "\n",
      "Epoch = 28\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.001764076081006533\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0018032822392975625\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0018729843654158678\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007977630213808371\n",
      "============================================================\n",
      "\n",
      "Epoch = 29\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0015539413775791378\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.001771527340099226\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0019250633366572683\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.00797683245078699\n",
      "============================================================\n",
      "\n",
      "Epoch = 30\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0016142699209902012\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0019821800546217763\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0018918506943364857\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007976034767541912\n",
      "============================================================\n",
      "\n",
      "Epoch = 31\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0018073659004216773\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.001556999008864412\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.001895297704930033\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007975237164065159\n",
      "============================================================\n",
      "\n",
      "Epoch = 32\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0019335340878663739\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0016483080439018746\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0020246993907802836\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007974439640348753\n",
      "============================================================\n",
      "\n",
      "Epoch = 33\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0016934051993193577\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0017988571870119021\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0018246086011117302\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007973642196384719\n",
      "============================================================\n",
      "\n",
      "Epoch = 34\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.001757196092911039\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0017154063429123969\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.002000994208020546\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.00797284483216508\n",
      "============================================================\n",
      "\n",
      "Epoch = 35\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0017158740009217885\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0016694532192267505\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0018559125130093316\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007972047547681863\n",
      "============================================================\n",
      "\n",
      "Epoch = 36\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0016722990749351288\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.002318891088416427\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0016390497463424623\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007971250342927096\n",
      "============================================================\n",
      "\n",
      "Epoch = 37\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0018780431537002305\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0018775460074662398\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.001698938516043169\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007970453217892803\n",
      "============================================================\n",
      "\n",
      "Epoch = 38\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0016607948721257967\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0016871035850933688\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0018655795336390015\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007969656172571013\n",
      "============================================================\n",
      "\n",
      "Epoch = 39\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.001490400952809425\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0018347609366300578\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.002018046532870168\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007968859206953756\n",
      "============================================================\n",
      "\n",
      "Epoch = 40\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.00168139893374562\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0015834304330886303\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0017110358876913255\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007968062321033061\n",
      "============================================================\n",
      "\n",
      "Epoch = 41\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0016890093807762764\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.001961312170837446\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0016190300420908998\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007967265514800958\n",
      "============================================================\n",
      "\n",
      "Epoch = 42\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0017882353699380927\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0018667252390657573\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0016585484328935007\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007966468788249479\n",
      "============================================================\n",
      "\n",
      "Epoch = 43\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0018387293658846436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0016655485224097563\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0016978443435634325\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007965672141370653\n",
      "============================================================\n",
      "\n",
      "Epoch = 44\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0017648583733127048\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0016713824276747662\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.002297185539260146\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007964875574156515\n",
      "============================================================\n",
      "\n",
      "Epoch = 45\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0018643977374515424\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0017640351732654412\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0017841101292624777\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.0079640790865991\n",
      "============================================================\n",
      "\n",
      "Epoch = 46\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.001706868251878259\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.002039534508183117\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0018409947012758758\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007963282678690441\n",
      "============================================================\n",
      "\n",
      "Epoch = 47\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.002018051488116686\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.001727570034991398\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0019559822921495986\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007962486350422572\n",
      "============================================================\n",
      "\n",
      "Epoch = 48\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0017092550680531619\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.002190072136972149\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0017860092673049452\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.00796169010178753\n",
      "============================================================\n",
      "\n",
      "Epoch = 49\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0017447860975006494\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0014835038223808344\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0019207008466131407\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007960893932777352\n",
      "============================================================\n",
      "\n",
      "Epoch = 50\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0017687937295285242\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0020131912768878695\n",
      "| => | Batch 201 |\n",
      "\n",
      "==> Batch 200 loss = 0.0017677833739854535\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007960097843384074\n"
     ]
    }
   ],
   "source": [
    "# test fourier net in 2d with cartesian product features\n",
    "# testing Fourier embedded net in 2d (old architecture)\n",
    "nn_fourier2d_cartesian = FourierProductEmbeddedDNN2d(\n",
    "    layers_time=[40, 200, 200, 200, 1], \n",
    "    layers_space=[40, 200, 200, 200, 1], \n",
    "    activation=torch.nn.Tanh, \n",
    "    last_layer_activation=None, \n",
    "    mt=20, \n",
    "    mx=20, \n",
    "    freq_stds_t=[1.,2.,10.,20.,50.], \n",
    "    freq_stds_x=[1.,2.,10.,20.,50.]\n",
    ")\n",
    "optim = torch.optim.Adam(\n",
    "    nn_fourier2d_cartesian.parameters(),\n",
    "    lr=8e-3\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.9999)\n",
    "train(X, y, nn_fourier2d_cartesian, optim, scheduler, 2**12, 50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e45926",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pred = nn_fourier2d_cartesian(X).reshape([len(new_xgrid[0]), len(new_tgrid[0])]).detach().numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5886944",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_exact = data[\"pmc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3f8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(p_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ec12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(p_exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31991353",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = -1\n",
    "plt.plot(p_pred[idx, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f703e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize = True\n",
    "if visualize:\n",
    "    for i in range(1):\n",
    "        plt.figure(1);\n",
    "        plt.plot(new_xgrid.flatten(), p_pred[i, :].flatten(), color=\"red\");\n",
    "        plt.plot(new_xgrid.flatten(), p_exact[i, :].flatten()/hello, color=\"blue\");\n",
    "        plt.title(r\"$t = {}$\".format(i*new_tgrid.flatten()[1]-new_tgrid.flatten()[0]));\n",
    "        display.clear_output(wait=True);\n",
    "        display.display(pl.gcf());\n",
    "        plt.clf();\n",
    "        time.sleep(0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b4ffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad71dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
