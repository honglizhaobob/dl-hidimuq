{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9753357",
   "metadata": {},
   "source": [
    "# Regression with Fourier Features\n",
    "\n",
    "Initially observed in: https://arxiv.org/abs/2006.10739\n",
    "\n",
    "The inclusion of Fourier features allows neural networks to capture high frequency information of the target function. In this short note, we compare a vanilla DNN with another Fourier-feature embedded DNN on the task of learning a (noise-perturbed) high frequency function.\n",
    "\n",
    "This notebook should be self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d068d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "from collections import OrderedDict\n",
    "\n",
    "# set random seeds\n",
    "np.random.seed(10)\n",
    "torch.manual_seed(10);\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "import numpy as np    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4737684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vanilla deep neural net\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, layers, \n",
    "        activation=torch.nn.Tanh, \n",
    "        last_layer_activation=None,\n",
    "        initialization=None\n",
    "    ):\n",
    "        \"\"\" \n",
    "            Custom initialization of neural network layers with the option \n",
    "            of changing the output layer's activation function.\n",
    "        \"\"\"\n",
    "        super(DNN, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        # set up layer order dict\n",
    "        self.activation = activation\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        if last_layer_activation is not None:\n",
    "            layer_list.append(\n",
    "            ('activation_%d' % (self.depth - 1), last_layer_activation())\n",
    "        )\n",
    "\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "        \n",
    "        # custom initialization modes\n",
    "        self.initialize(mode=initialization)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def initialize(self, mode):\n",
    "        if mode == None:\n",
    "            return\n",
    "        else:\n",
    "            for layer in self.layers:\n",
    "                if isinstance(layer, torch.nn.Linear):\n",
    "                    # initialize depending on mode\n",
    "                    if mode == \"xavier\":\n",
    "                        torch.nn.init.xavier_uniform_(layer.weight)\n",
    "                    elif mode == \"kaiming\":\n",
    "                        torch.nn.init.kaiming_uniform_(layer.weight)\n",
    "                    elif mode == \"normal\":\n",
    "                        torch.nn.init.normal_(layer.weight)\n",
    "                    elif mode == \"uniform\":\n",
    "                        torch.nn.init.uniform_(layer.weight)\n",
    "                    elif mode == \"ones\":\n",
    "                        torch.nn.init.ones_(layer.weight)\n",
    "                    else:\n",
    "                        raise NotImplementedError()\n",
    "            return\n",
    "        \n",
    "class FourierEmbeddedDNN(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 layers, \n",
    "                 activation=torch.nn.Tanh, \n",
    "                 last_layer_activation=None, \n",
    "                 initialization=None,\n",
    "                 m=1,\n",
    "                 freq_stds=None):\n",
    "        super(FourierEmbeddedDNN, self).__init__()\n",
    "        # fourier embedding is applied prior to passing into neural net, \n",
    "        # need to make sure dimensions match\n",
    "        assert layers[0] == 2*m\n",
    "        # build main DNN\n",
    "        self.layer_spec = layers\n",
    "        self.layers = self.build_nn(\n",
    "            layers, activation, last_layer_activation, initialization\n",
    "        )\n",
    "        # build fourier feature embedding\n",
    "        self.fourier_embedding = self.build_embedding(m, freq_stds)\n",
    "        \n",
    "        # build final aggregator to combine outputs of different scale fourier embeddings\n",
    "        self.build_aggregator()\n",
    "    \n",
    "    def build_nn(self, layers, activation, last_layer_activation, initialization):\n",
    "        self.depth = len(layers) - 1\n",
    "        # set up layer order dict\n",
    "        self.activation = activation\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        if last_layer_activation is not None:\n",
    "            layer_list.append(\n",
    "            ('activation_%d' % (self.depth - 1), last_layer_activation())\n",
    "        )\n",
    "\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        return torch.nn.Sequential(layerDict)\n",
    "    \n",
    "    def build_embedding(self, num_freqs, freq_stds):\n",
    "        # number of feature embeddings correspond to length of standard \n",
    "        # deviations specified. If `None`, by default uses only 1 embedding\n",
    "        # standard Gaussian.\n",
    "        if freq_stds:\n",
    "            self.num_embeddings = len(freq_stds)\n",
    "        else:\n",
    "            self.num_embeddings = 1\n",
    "            freq_stds = [1.0]\n",
    "        # draw frequency matrix\n",
    "        freq_matrix = [torch.randn(num_freqs, requires_grad=False) for _ in range(self.num_embeddings)]\n",
    "        for i in range(self.num_embeddings):\n",
    "            # scale by frequency standard deviation\n",
    "            freq_matrix[i] = torch.tensor(freq_stds[i])*freq_matrix[i]\n",
    "        return freq_matrix\n",
    "    \n",
    "    def build_aggregator(self):\n",
    "        # number of fourier embeddings\n",
    "        k = self.num_embeddings\n",
    "        # size of hidden layer final outputs\n",
    "        num_out = self.layer_spec[-1]\n",
    "        # create trainable aggregating weights for each embedding (simple linear aggregation\n",
    "        # , may also consider computing another nonlinear activation for each embedding, then \n",
    "        # summing all outputs).\n",
    "        self.aggregator = torch.nn.Linear(num_out*k, 1)\n",
    "        \n",
    "    def fourier_lifting(self, x, freq):\n",
    "        # input x has size (N x 1), output has size (N x 2*m) where m is number of Fourier bases\n",
    "        \n",
    "        # has size (N x m)\n",
    "        x = freq * x\n",
    "        # lift to sin and cos space\n",
    "        x = torch.concat(\n",
    "            [\n",
    "                torch.cos(2*torch.pi*x), \n",
    "                torch.sin(2*torch.pi*x)\n",
    "            ], dim=1\n",
    "        )\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # inputs x has size (N x 1)\n",
    "        # create Fourier features\n",
    "        lifted = []\n",
    "        for i in range(self.num_embeddings):\n",
    "            lifted.append(self.fourier_lifting(x, self.fourier_embedding[i]))\n",
    "        # lifted is a length-k list of (N x 2*m) tensors of lifted features according to \n",
    "        # k different scales.\n",
    "        \n",
    "        # now pass each (N x 2*m) features into the hidden layers\n",
    "        for i in range(self.num_embeddings):\n",
    "            lifted[i] = self.layers(lifted[i])\n",
    "        \n",
    "        # lifted is a length-k list of (N x num_out) tensor of transformed fourier features\n",
    "        # now concatenate into (N x num_out*k) and pass into aggregator to obtain (N x 1) prediction\n",
    "        lifted = torch.concat(lifted, dim=1)\n",
    "        # final aggregation\n",
    "        lifted = self.aggregator(lifted)\n",
    "        return lifted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a10ffe",
   "metadata": {},
   "source": [
    "Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c02e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# oscillations with decay\n",
    "#f = lambda x: 100*(np.exp(-2.0*x)*np.sin(10*np.pi*x))+np.cos(5.0*np.pi*x)\n",
    "\n",
    "# different frequencies\n",
    "#f = lambda x: np.sin(2*np.pi*x) + np.cos(10.*np.pi*x) + 2.0*np.sin(15.*np.pi*x) + np.sin(100.0*np.pi*x)\n",
    "\n",
    "# linear function\n",
    "#f = lambda x: 10.0*(x**2)\n",
    "\n",
    "# 1/x\n",
    "f = lambda x: np.sin(10.0*np.pi/x)\n",
    "x_start, x_end = 0.01, 1.0\n",
    "nx = 2048\n",
    "xgrid = np.linspace(x_start, x_end, nx)\n",
    "dx = xgrid[1]-xgrid[0]\n",
    "# visualize function with noise perturbation\n",
    "f_data = f(xgrid)\n",
    "noise_level = 0.0\n",
    "f_data = f_data + noise_level * np.random.randn(f_data.shape[0])\n",
    "plt.plot(xgrid, f_data, \"--\", color=\"red\", lw=3.0, label=\"exact\", alpha=0.6);\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a12a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "inputs = torch.tensor(xgrid).reshape(-1, 1)\n",
    "outputs = torch.tensor(f_data).reshape(-1, 1)\n",
    "\n",
    "# define training function\n",
    "def train(inputs, outputs, model, optim, scheduler, batch_size, epochs, shuffle=True):\n",
    "    X, y = inputs, outputs\n",
    "    nx = X.shape[0]\n",
    "    num_batches = int(nx/batch_size)\n",
    "    for i in range(epochs):\n",
    "        print(\"============================================================\\n\")\n",
    "        print(\"Epoch = {}\\n\".format(i+1));\n",
    "        print(\"============================================================\\n\")\n",
    "        model.train()\n",
    "        if shuffle:\n",
    "            tmp = np.random.permutation(nx)\n",
    "            X, y = X[tmp, :].data.clone(), y[tmp, :].data.clone()\n",
    "        for idx in range(num_batches):\n",
    "            print(\"| => | Batch {} |\\n\".format(idx+1))\n",
    "        # closure definition\n",
    "            def closure():\n",
    "                optim.zero_grad()\n",
    "                start_idx = idx*batch_size\n",
    "                end_idx = (idx+1)*batch_size\n",
    "                if idx + 1 == num_batches:\n",
    "                    # if last batch\n",
    "                    end_idx = -1\n",
    "                Xb, yb = X[start_idx:end_idx, :].data.clone(), y[start_idx:end_idx, :].data.clone()\n",
    "\n",
    "                # require gradients\n",
    "                Xb.requires_grad = True\n",
    "                # make a prediction on the batch\n",
    "                y_pred = model.forward(Xb)\n",
    "                # compute L^2 loss\n",
    "                loss = torch.mean((y_pred - yb)**2)\n",
    "                # backpropagate\n",
    "                loss.backward()\n",
    "                print(\"==> Batch {} loss = {}\".format(idx, loss.item()))\n",
    "                return loss\n",
    "            optim.step(closure=closure)\n",
    "        if scheduler:\n",
    "            # step scheduler after epoch if there is one\n",
    "            scheduler.step()\n",
    "            print(\"---------- \\n\")\n",
    "            print(\"++ Learning rate reduced, now at = {}\".format(scheduler.get_last_lr()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be654f9",
   "metadata": {},
   "source": [
    "Testing neural net performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea2549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanilla PINN: initialize optimizer and scheduler\n",
    "nn_vanilla = DNN(layers=[1, 128, 128, 128, 1])\n",
    "optim = torch.optim.Adam(\n",
    "    nn_vanilla.parameters(),\n",
    "    lr=8e-3\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.9999)\n",
    "train(inputs, outputs, nn_vanilla, optim, scheduler, 2**10, 500, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0478710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test fourier net\n",
    "nn_fourier = FourierEmbeddedDNN(\n",
    "    layers=[30, 128, 128, 128, 1],\n",
    "    m=15, \n",
    "    freq_stds=[1.,2.,5.,10.,20.,50.,60.,70.,80.,90.,100,]\n",
    ")\n",
    "optim = torch.optim.Adam(\n",
    "    nn_fourier.parameters(),\n",
    "    lr=8e-3\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.9999)\n",
    "train(inputs, outputs, nn_fourier, optim, scheduler, 2**10, 500, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ad36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(15, 5))\n",
    "# interpolate on finer grid\n",
    "x_start, x_end = 0.01, 1.0\n",
    "nx = 2048\n",
    "xgrid = np.linspace(x_start, x_end, nx)\n",
    "inputs = torch.tensor(xgrid).reshape(-1, 1)\n",
    "plt.plot(xgrid, nn_fourier(inputs).detach().numpy().flatten(), color=\"black\", label=\"Fourier Embedded\");\n",
    "plt.plot(xgrid, nn_vanilla(inputs).detach().numpy().flatten(), lw=2.0, color='blue', label=\"Vanilla\");\n",
    "plt.plot(xgrid, f(xgrid), lw=6.0, alpha=0.4, color=\"red\", label=\"exact\");\n",
    "plt.legend();\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc5d9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
