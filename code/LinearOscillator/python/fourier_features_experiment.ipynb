{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e488e830",
   "metadata": {},
   "source": [
    "# Regression with Fourier Features\n",
    "\n",
    "Initially observed in: https://arxiv.org/abs/2006.10739\n",
    "\n",
    "The inclusion of Fourier features allows neural networks to capture high frequency information of the target function. In this short note, we compare a vanilla DNN with another Fourier-feature embedded DNN on the task of learning a (noise-perturbed) high frequency function.\n",
    "\n",
    "This notebook should be self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "269bb5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "from collections import OrderedDict\n",
    "\n",
    "# set random seeds\n",
    "np.random.seed(10)\n",
    "torch.manual_seed(10);\n",
    "\n",
    "import numpy as np    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "626c3815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vanilla deep neural net\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, layers, \n",
    "        activation=torch.nn.ReLU, \n",
    "        last_layer_activation=torch.nn.ReLU,\n",
    "        initialization=None\n",
    "    ):\n",
    "        \"\"\" \n",
    "            Custom initialization of neural network layers with the option \n",
    "            of changing the output layer's activation function.\n",
    "        \"\"\"\n",
    "        super(DNN, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        # set up layer order dict\n",
    "        self.activation = activation\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        if last_layer_activation is not None:\n",
    "            layer_list.append(\n",
    "            ('activation_%d' % (self.depth - 1), last_layer_activation())\n",
    "        )\n",
    "\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "        \n",
    "        # custom initialization modes\n",
    "        self.initialize(mode=initialization)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def initialize(self, mode):\n",
    "        if mode == None:\n",
    "            return\n",
    "        else:\n",
    "            for layer in self.layers:\n",
    "                if isinstance(layer, torch.nn.Linear):\n",
    "                    # initialize depending on mode\n",
    "                    if mode == \"xavier\":\n",
    "                        torch.nn.init.xavier_uniform_(layer.weight)\n",
    "                    elif mode == \"kaiming\":\n",
    "                        torch.nn.init.kaiming_uniform_(layer.weight)\n",
    "                    elif mode == \"normal\":\n",
    "                        torch.nn.init.normal_(layer.weight)\n",
    "                    elif mode == \"uniform\":\n",
    "                        torch.nn.init.uniform_(layer.weight)\n",
    "                    elif mode == \"ones\":\n",
    "                        torch.nn.init.ones_(layer.weight)\n",
    "                    else:\n",
    "                        raise NotImplementedError()\n",
    "            return\n",
    "        \n",
    "class FourierEmbeddedDNN(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 layers, \n",
    "                 activation=torch.nn.ReLU, \n",
    "                 last_layer_activation=torch.nn.ReLU, \n",
    "                 initialization=None,\n",
    "                 m=1,\n",
    "                 freq_stds=None):\n",
    "        super(FourierEmbeddedDNN, self).__init__()\n",
    "        # fourier embedding is applied prior to passing into neural net, \n",
    "        # need to make sure dimensions match\n",
    "        assert layers[0] == 2*m\n",
    "        # build main DNN\n",
    "        self.layer_spec = layers\n",
    "        self.layers = self.build_nn(\n",
    "            layers, activation, last_layer_activation, initialization\n",
    "        )\n",
    "        # build fourier feature embedding\n",
    "        self.fourier_embedding = self.build_embedding(m, freq_stds)\n",
    "        \n",
    "        # build final aggregator to combine outputs of different scale fourier embeddings\n",
    "        self.build_aggregator()\n",
    "    \n",
    "    def build_nn(self, layers, activation, last_layer_activation, initialization):\n",
    "        self.depth = len(layers) - 1\n",
    "        # set up layer order dict\n",
    "        self.activation = activation\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        if last_layer_activation is not None:\n",
    "            layer_list.append(\n",
    "            ('activation_%d' % (self.depth - 1), last_layer_activation())\n",
    "        )\n",
    "\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        return torch.nn.Sequential(layerDict)\n",
    "    \n",
    "    def build_embedding(self, num_freqs, freq_stds):\n",
    "        # number of feature embeddings correspond to length of standard \n",
    "        # deviations specified. If `None`, by default uses only 1 embedding\n",
    "        # standard Gaussian.\n",
    "        if freq_stds:\n",
    "            self.num_embeddings = len(freq_stds)\n",
    "        else:\n",
    "            self.num_embeddings = 1\n",
    "            freq_stds = [1.0]\n",
    "        # draw frequency matrix\n",
    "        freq_matrix = [torch.randn(num_freqs, requires_grad=False) for _ in range(self.num_embeddings)]\n",
    "        for i in range(self.num_embeddings):\n",
    "            # scale by frequency standard deviation\n",
    "            freq_matrix[i] = torch.tensor(freq_stds[i])*freq_matrix[i]\n",
    "        return freq_matrix\n",
    "    \n",
    "    def build_aggregator(self):\n",
    "        # number of fourier embeddings\n",
    "        k = self.num_embeddings\n",
    "        # size of hidden layer final outputs\n",
    "        num_out = self.layer_spec[-1]\n",
    "        # create trainable aggregating weights for each embedding (simple linear aggregation\n",
    "        # , may also consider computing another nonlinear activation for each embedding, then \n",
    "        # summing all outputs).\n",
    "        self.aggregator = torch.nn.Linear(num_out*k, 1)\n",
    "        \n",
    "    def fourier_lifting(self, x, freq):\n",
    "        # input x has size (N x 1), output has size (N x 2*m) where m is number of Fourier bases\n",
    "        \n",
    "        # has size (N x m)\n",
    "        x = freq * x\n",
    "        # lift to sin and cos space\n",
    "        x = torch.concat(\n",
    "            [\n",
    "                torch.cos(2*torch.pi*x), \n",
    "                torch.sin(2*torch.pi*x)\n",
    "            ], dim=1\n",
    "        )\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # inputs x has size (N x 1)\n",
    "        # create Fourier features\n",
    "        lifted = []\n",
    "        for i in range(self.num_embeddings):\n",
    "            lifted.append(self.fourier_lifting(x, self.fourier_embedding[i]))\n",
    "        # lifted is a length-k list of (N x 2*m) tensors of lifted features according to \n",
    "        # k different scales.\n",
    "        \n",
    "        # now pass each (N x 2*m) features into the hidden layers\n",
    "        for i in range(self.num_embeddings):\n",
    "            lifted[i] = self.layers(lifted[i])\n",
    "        \n",
    "        # lifted is a length-k list of (N x num_out) tensor of transformed fourier features\n",
    "        # now concatenate into (N x num_out*k) and pass into aggregator to obtain (N x 1) prediction\n",
    "        lifted = torch.concat(lifted, dim=1)\n",
    "        # final aggregation\n",
    "        lifted = self.aggregator(lifted)\n",
    "        return lifted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534783b1",
   "metadata": {},
   "source": [
    "Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "eca7eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e31cd",
   "metadata": {},
   "source": [
    "Testing neural net performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "279618d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_vanilla = DNN(layers=[1, 128, 128, 128, 1], initialization=\"xavier\")\n",
    "nn_fourier = FourierEmbeddedDNN(\n",
    "    layers=[10, 128, 128, 128, 1],\n",
    "    m=5, \n",
    "    freq_stds=[1., 2., 5., 10., 20., 50., 100.]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4f22f6",
   "metadata": {},
   "source": [
    "Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f7c111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
