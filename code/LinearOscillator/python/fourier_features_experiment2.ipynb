{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05651ff2",
   "metadata": {},
   "source": [
    "# Regression with Fourier Features - Part 2\n",
    "\n",
    "Initially observed in: https://arxiv.org/abs/2006.10739\n",
    "\n",
    "The inclusion of Fourier features allows neural networks to capture high frequency information of the target function. In this short note, we compare a vanilla DNN with another Fourier-feature embedded DNN on the task of learning a (noise-perturbed) PDE solution in both space and time domains. The Fourier features embedding is done in both directions and results are aggregated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1987ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "from collections import OrderedDict\n",
    "\n",
    "# set random seeds\n",
    "np.random.seed(10)\n",
    "torch.manual_seed(10);\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "import numpy as np    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# import neural nets\n",
    "from PINN.utils.dnn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d72af32",
   "metadata": {},
   "source": [
    "Generate data from the following PDE:\n",
    "$$\n",
    "    \\frac{\\partial u}{\\partial t} + u_0\\sin(\\omega t)\\frac{\\partial u}{\\partial x} = 0\n",
    "$$ whose analytical solution is given by:\n",
    "$$\n",
    "    u(t,x) = \\exp\\bigg(\n",
    "            -k[x - x_0 - 2v_0\\omega^{-1}\\sin^2(\\frac{1}{2}\\omega t)]^2\n",
    "        \\bigg)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac076416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1048, 500)\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "x0 = 2.0\n",
    "v0 = 2.0\n",
    "k = 5.0\n",
    "omega = 2.0*np.pi\n",
    "# time grid\n",
    "t_start = 0.0\n",
    "t_end = 2*np.pi\n",
    "dt = 0.001\n",
    "tgrid = np.arange(t_start, t_end, dt)\n",
    "nt = len(tgrid)\n",
    "# spatial grid\n",
    "x_left, x_right = 0.0, 5.0\n",
    "dx = 0.005\n",
    "xgrid = np.arange(x_left, x_right, dx)\n",
    "nx = len(xgrid)\n",
    "\n",
    "# solution\n",
    "u_sol = np.zeros([nt, nx])\n",
    "for i in range(nt):\n",
    "    t = tgrid[i]\n",
    "    u_sol[i, :] = np.exp(-k * (( xgrid - x0 ) - (2*v0/omega) * (np.sin(0.5*omega*t) ** 2)) ** 2 )\n",
    "\n",
    "# subsample grids\n",
    "subsample_t = 6\n",
    "subsample_x = 2\n",
    "xgrid_small = xgrid.reshape(1, -1)[:, 0:-1:subsample_x].squeeze(),\n",
    "tgrid_small = tgrid.reshape(1, -1)[:, 0:-1:subsample_t].squeeze()\n",
    "u_sol_small = u_sol[0:-1:subsample_t, 0:-1:subsample_x]\n",
    "print(u_sol_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f336afae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1);\n",
    "plt.contourf(u_sol_small);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ff8df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data\n",
    "X = cartesian_data(torch.tensor(tgrid_small.flatten()), torch.tensor(xgrid_small[0].flatten()))\n",
    "y = torch.tensor(u_sol_small).T.flatten().reshape(-1, 1)\n",
    "#y = torch.tensor(u_sol_small2).T.flatten().reshape(-1, 1)\n",
    "\n",
    "# define training function\n",
    "def train(inputs, outputs, model, optim, scheduler, batch_size, epochs, shuffle=True):\n",
    "    X, y = inputs, outputs\n",
    "    nx = X.shape[0]\n",
    "    num_batches = int(nx/batch_size)\n",
    "    for i in range(epochs):\n",
    "        print(\"============================================================\\n\")\n",
    "        print(\"Epoch = {}\\n\".format(i+1));\n",
    "        print(\"============================================================\\n\")\n",
    "        model.train()\n",
    "        if shuffle:\n",
    "            tmp = np.random.permutation(nx)\n",
    "            X, y = X[tmp, :].data.clone(), y[tmp, :].data.clone()\n",
    "        for idx in range(num_batches):\n",
    "            if idx % 100 == 0:\n",
    "                print(\"| => | Batch {} |\\n\".format(idx+1))\n",
    "        # closure definition\n",
    "            def closure():\n",
    "                optim.zero_grad()\n",
    "                start_idx = idx*batch_size\n",
    "                end_idx = (idx+1)*batch_size\n",
    "                if idx + 1 == num_batches:\n",
    "                    # if last batch\n",
    "                    end_idx = -1\n",
    "                Xb, yb = X[start_idx:end_idx, :].data.clone(), y[start_idx:end_idx, :].data.clone()\n",
    "\n",
    "                # require gradients\n",
    "                Xb.requires_grad = True\n",
    "                # make a prediction on the batch\n",
    "                y_pred = model.forward(Xb)\n",
    "                # compute L^2 loss\n",
    "                loss = torch.mean((y_pred - yb)**2)\n",
    "                # backpropagate\n",
    "                loss.backward()\n",
    "                if idx % 100 == 0:\n",
    "                    print(\"==> Batch {} loss = {}\".format(idx, loss.item()))\n",
    "                return loss\n",
    "            optim.step(closure=closure)\n",
    "        if scheduler:\n",
    "            # step scheduler after epoch if there is one\n",
    "            scheduler.step()\n",
    "            print(\"---------- \\n\")\n",
    "            print(\"++ Learning rate reduced, now at = {}\".format(scheduler.get_last_lr()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test vanilla neural net\n",
    "nn_vanilla = DNN(layers=[2, 32, 32, 1])\n",
    "nn_vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d3d570",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(\n",
    "    nn_vanilla.parameters(),\n",
    "    lr=8e-3\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.9999)\n",
    "train(X, y, nn_vanilla, optim, scheduler, 2**12, 50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea6c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions and compare contour plots\n",
    "nt_small = len(tgrid_small.flatten())\n",
    "nx_small = len(xgrid_small[0].flatten())\n",
    "u_sol_predict = nn_vanilla(X).reshape([nx_small, nt_small]).detach().numpy().T\n",
    "u_sol_exact = y.reshape([nx_small, nt_small]).detach().numpy().T\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].contourf(u_sol_predict);\n",
    "ax[1].contourf(u_sol_exact);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad91c9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "Epoch = 1\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.40578627770075537\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m optim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\n\u001b[1;32m     10\u001b[0m     nn_fourier2d\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m     11\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8e-3\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mExponentialLR(optim, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9999\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn_fourier2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(inputs, outputs, model, optim, scheduler, batch_size, epochs, shuffle)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==> Batch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m loss = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(idx, loss\u001b[38;5;241m.\u001b[39mitem()))\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m---> 43\u001b[0m     \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scheduler:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# step scheduler after epoch if there is one\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py:118\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 118\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    121\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mtrain.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m Xb\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# make a prediction on the batch\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# compute L^2 loss\u001b[39;00m\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((y_pred \u001b[38;5;241m-\u001b[39m yb)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/dl-hidimuq/code/LinearOscillator/python/PINN/utils/dnn.py:346\u001b[0m, in \u001b[0;36mFourierEmbeddedDNN2d.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    344\u001b[0m     lifted_x[i, :, :] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfourier_lifting(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfourier_embedding_space[:, i])\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# pass through deep neural network (num_embeddings x n x 2m) => (num_embeddings x n x k)\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m lifted_t, lifted_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlifted_t\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers(lifted_x)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# form hadamard product (num_embedding_t*num_embedding_x x n x k)\u001b[39;00m\n\u001b[1;32m    349\u001b[0m lifted \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/activation.py:354\u001b[0m, in \u001b[0;36mTanh.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# testing Fourier embedded net in 2d\n",
    "nn_fourier2d = FourierEmbeddedDNN2d(\n",
    "    200,\n",
    "    4, \n",
    "    1,\n",
    "    m=15, \n",
    "    freq_stds={\"time\": [1.0, 2.0, 5.0, 10.0], \"space\": [1.0]}\n",
    ")\n",
    "optim = torch.optim.Adam(\n",
    "    nn_fourier2d.parameters(),\n",
    "    lr=8e-3\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.9999)\n",
    "train(X, y, nn_fourier2d, optim, scheduler, 2**12, 50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82748b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "nt_small = len(tgrid_small.flatten())\n",
    "nx_small = len(xgrid_small[0].flatten())\n",
    "u_sol_predict_fourier = nn_fourier2d(X).reshape([nx_small, nt_small]).detach().numpy().T\n",
    "u_sol_exact_fourier = y.reshape([nx_small, nt_small]).detach().numpy().T\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].contourf(u_sol_predict_fourier)\n",
    "ax[1].contourf(u_sol_exact_fourier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0614406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((u_sol_predict_fourier-u_sol_exact_fourier)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f3ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing Fourier embedded net in 2d (old architecture)\n",
    "nn_fourier2d_old = FourierEmbeddedDNN2dOld(\n",
    "    layers=[40, 128, 128, 128, 1],\n",
    "    m=20, \n",
    "    freq_stds=np.array([[1.,2.,10.,20.,100.], [1.,2.,10.,20.,100.]]).T\n",
    ")\n",
    "optim = torch.optim.Adam(\n",
    "    nn_fourier2d_old.parameters(),\n",
    "    lr=8e-3\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.9999)\n",
    "train(X, y, nn_fourier2d_old, optim, scheduler, 2**12, 50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0132013",
   "metadata": {},
   "outputs": [],
   "source": [
    "nt_small = len(tgrid_small.flatten())\n",
    "nx_small = len(xgrid_small[0].flatten())\n",
    "u_sol_predict_fourier = nn_fourier2d_old(X).reshape([nx_small, nt_small]).detach().numpy().T\n",
    "u_sol_exact_fourier = y.reshape([nx_small, nt_small]).detach().numpy().T\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].contourf(u_sol_predict_fourier)\n",
    "ax[1].contourf(u_sol_exact_fourier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e251ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((u_sol_predict_fourier-u_sol_exact_fourier)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7cdcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "Epoch = 1\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.08912206201760287\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.001948770304029966\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.0079992\n",
      "============================================================\n",
      "\n",
      "Epoch = 2\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0017076197832130591\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0015311062680269595\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007998400079999999\n",
      "============================================================\n",
      "\n",
      "Epoch = 3\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.001491102267604817\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0015227954503982833\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007997600239991999\n",
      "============================================================\n",
      "\n",
      "Epoch = 4\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0015299538743800863\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.001527301510376997\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007996800479968\n",
      "============================================================\n",
      "\n",
      "Epoch = 5\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0014546211674971\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0013933074412001163\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007996000799920003\n",
      "============================================================\n",
      "\n",
      "Epoch = 6\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0013990468887482646\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0013824977417878997\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007995201199840011\n",
      "============================================================\n",
      "\n",
      "Epoch = 7\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0013782867637927368\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0012167051734302357\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007994401679720027\n",
      "============================================================\n",
      "\n",
      "Epoch = 8\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.00126859289813146\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0010553089720511124\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007993602239552054\n",
      "============================================================\n",
      "\n",
      "Epoch = 9\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.000639287775291264\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0002883651099483514\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007992802879328098\n",
      "============================================================\n",
      "\n",
      "Epoch = 10\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.00024622133147444644\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.000250998827584852\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007992003599040166\n",
      "============================================================\n",
      "\n",
      "Epoch = 11\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.00025138158976919535\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.00014034793173021236\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007991204398680262\n",
      "============================================================\n",
      "\n",
      "Epoch = 12\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.0001141410672369161\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.00011877812992878279\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007990405278240395\n",
      "============================================================\n",
      "\n",
      "Epoch = 13\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.00016122069149637929\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0001341244815619318\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007989606237712572\n",
      "============================================================\n",
      "\n",
      "Epoch = 14\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.00014795413740924967\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.00011025183407698314\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.0079888072770888\n",
      "============================================================\n",
      "\n",
      "Epoch = 15\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.00011338099729816896\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.00010959331069654567\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007988008396361091\n",
      "============================================================\n",
      "\n",
      "Epoch = 16\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.00010565907588976959\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.00012484236052404918\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007987209595521455\n",
      "============================================================\n",
      "\n",
      "Epoch = 17\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.00016085148131669575\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.00014048425658952997\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007986410874561903\n",
      "============================================================\n",
      "\n",
      "Epoch = 18\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 9.334790666528872e-05\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.00014241801897782583\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007985612233474448\n",
      "============================================================\n",
      "\n",
      "Epoch = 19\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 0.00011203181228762761\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 8.432252544757388e-05\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.0079848136722511\n",
      "============================================================\n",
      "\n",
      "Epoch = 20\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 9.659980645200542e-05\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.00011456758036763995\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007984015190883875\n",
      "============================================================\n",
      "\n",
      "Epoch = 21\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 8.982924034947121e-05\n",
      "| => | Batch 101 |\n",
      "\n",
      "==> Batch 100 loss = 0.0001451098012465952\n",
      "---------- \n",
      "\n",
      "++ Learning rate reduced, now at = 0.007983216789364787\n",
      "============================================================\n",
      "\n",
      "Epoch = 22\n",
      "\n",
      "============================================================\n",
      "\n",
      "| => | Batch 1 |\n",
      "\n",
      "==> Batch 0 loss = 9.25373699616793e-05\n"
     ]
    }
   ],
   "source": [
    "# test fourier net in 2d with cartesian product features\n",
    "# testing Fourier embedded net in 2d (old architecture)\n",
    "nn_fourier2d_cartesian = FourierProductEmbeddedDNN2d(\n",
    "    layers_time=[40, 128, 128, 128, 1], \n",
    "    layers_space=[40, 128, 128, 128, 1], \n",
    "    activation=torch.nn.Tanh, \n",
    "    last_layer_activation=None, \n",
    "    mt=20, \n",
    "    mx=20, \n",
    "    freq_stds_t=[1.,2.,10.,20.,100.], \n",
    "    freq_stds_x=[1.,2.,10.,20.,100.]\n",
    ")\n",
    "optim = torch.optim.Adam(\n",
    "    nn_fourier2d_cartesian.parameters(),\n",
    "    lr=8e-3\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.9999)\n",
    "train(X, y, nn_fourier2d_cartesian, optim, scheduler, 2**12, 50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ab5ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nt_small = len(tgrid_small.flatten())\n",
    "nx_small = len(xgrid_small[0].flatten())\n",
    "u_sol_predict_fourier = nn_fourier2d_cartesian(X).reshape([nx_small, nt_small]).detach().numpy().T\n",
    "u_sol_exact_fourier = y.reshape([nx_small, nt_small]).detach().numpy().T\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].contourf(u_sol_predict_fourier)\n",
    "ax[1].contourf(u_sol_exact_fourier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c9cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_fourier2d_cartesian(X[0:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ea243",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = nn_fourier2d_cartesian.fourier_net_t.fourier_lifting(X[0:5, 0][:, None], nn_fourier2d_cartesian.fourier_net_t.fourier_embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810135c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_fourier2d_cartesian.fourier_net_t.layers(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1442151",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_fourier2d_cartesian.fourier_net_t.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc5e423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5ee36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = tgrid_small[1]-tgrid_small[0]\n",
    "for idx in range(nt_small):\n",
    "    if idx % 5 == 0:\n",
    "        plt.figure(1);\n",
    "        plt.plot(xgrid_small[0].flatten(), u_sol_predict_fourier[idx, :], lw=2., color=\"black\");\n",
    "        plt.plot(xgrid_small[0].flatten(), u_sol_exact_fourier[idx, :], \"--\", lw=8.0, color=\"red\", alpha=0.6);\n",
    "        #plt.plot(xgrid_small[0].flatten(), u_sol_predict[idx, :], lw=2., color=\"blue\", alpha=0.5);\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(pl.gcf())\n",
    "        plt.clf()\n",
    "        plt.title(r\"$t = {}$\".format(dt*(idx+1)))\n",
    "        time.sleep(0.01);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6023fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a49b7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e699d902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd316449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5918fde2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
